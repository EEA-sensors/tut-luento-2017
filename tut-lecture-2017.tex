\documentclass[first=dgreen,second=purple,presentation]{elecslides}
%\documentclass{elecslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES

\usepackage[latin9]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{url}
\usepackage{animate}
\usepackage{media9}
\usepackage{tabularx}
\usepackage{colortbl}

\usepackage{lcg}
\reinitrand[first=1, last=3, seed=-1]
\newcommand{\alogoname}[1]{Aalto_ELEC_EN_13_RGB_#1.png}

\rand
%\renewcommand{\smalllogo}{\includegraphics[height=\smalllogoheight]{\alogoname{\arabic{rand}}}}
\renewcommand{\smalllogo}{\includegraphics[height=\smalllogoheight]{\alogoname{1}}}

\graphicspath{{.}{../pics/}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mathbold}[1]{\bm{#1}}


\newcommand{\T}[0]{\mathsf{T}}

%\newcommand{\diff}[0]{\mathrm{d}}
\newcommand*{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\Dt}[0]{\delta t}
\newcommand{\Dx}[0]{\delta \vec{x}}
\newcommand{\Eg}[0]{\EuScript{E}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\chol}{chol}
\DeclareMathOperator{\dchol}{dchol}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\gammad}{Gamma}
\DeclareMathOperator{\expd}{Exp}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\U}{U}

\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\vchi}[0]{\mathbold{\chi}}
\newcommand{\vepsilon}[0]{\mathbold{\varepsilon}}
\newcommand{\veta}[0]{\mathbold{\eta}}
\newcommand{\vmu}[0]{\mathbold{\mu}}
\newcommand{\vxi}[0]{\mathbold{\xi}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\vTheta}[0]{\mathbold{\Theta}}
\newcommand{\vzeta}[0]{\mathbold{\zeta}}
\newcommand{\MPsi}[0]{\mathbold{\Psi}}
\newcommand{\MPhi}[0]{\mathbold{\Phi}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}

\newcommand{\RL}{\mathrm{L}}
\newcommand{\RM}{\mathrm{M}}
\newcommand{\RQ}{\mathrm{Q}}
\newcommand{\RS}{\mathrm{S}}
\newcommand{\RU}{\mathrm{U}}

\newcommand{\va}{\mbf{a}}
\newcommand{\vb}{\mbf{b}}
\newcommand{\vc}{\mbf{c}}
\newcommand{\vd}{\mbf{d}}
\newcommand{\ve}{\mbf{e}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vg}{\mbf{g}}
\newcommand{\vh}{\mbf{h}}
\newcommand{\vi}{\mbf{i}}
\newcommand{\vj}{\mbf{j}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\vl}{\mbf{l}}
\newcommand{\vm}{\mbf{m}}
\newcommand{\vn}{\mbf{n}}
\newcommand{\vo}{\mbf{o}}
\newcommand{\vp}{\mbf{p}}
\newcommand{\vq}{\mbf{q}}
\newcommand{\vr}{\mbf{r}}
\newcommand{\vs}{\mbf{s}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vv}{\mbf{v}}
\newcommand{\vw}{\mbf{w}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vz}{\mbf{z}}

\newcommand{\MA}{\mbf{A}}
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MD}{\mbf{D}}
\newcommand{\MF}{\mbf{F}}
\newcommand{\MG}{\mbf{G}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MJ}{\mbf{J}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\ML}{\mbf{L}}
\newcommand{\MM}{\mbf{M}}
\newcommand{\MP}{\mbf{P}}
\newcommand{\MQ}{\mbf{Q}}
\newcommand{\MR}{\mbf{R}}
\newcommand{\MS}{\mbf{S}}
\newcommand{\MT}{\mbf{T}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MY}{\mbf{Y}}


\mode<presentation>
{
  \setbeamercovered{transparent}
}

\mode<handout>
{
  %\beamertemplatesolidbackgroundcolor{black!5}
}

\title{Statistical linearization and statistical linear regression}

\aaltofootertext{Statistical linearization and statistical linear regression}{Simo S\"arkk\"a}{\insertframenumber\,/\,\inserttotalframenumber\ }

\author{{\bf Simo S\"arkk\"a}}

\institute{Aalto University, Finland}

\date{}

%\date{Version 1.0, \today}

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Learning Outcomes}
    \tableofcontents[currentsection]
  \end{frame}
}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aaltotitleframe

\begin{frame}
  \frametitle{Learning Outcomes}
  \tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical linearization}

\begin{frame}
 \frametitle{Linear approximation of a non-linearity}

\begin{itemize}[<+->]
\item We wish to approximate the \alert{transformation}
%
\begin{equation}
\begin{split}
  \vy = \vg(\vx).
\end{split}
\nonumber
\end{equation}

\item We choose the approximation to be a \alert{linear function}, say
%
\begin{equation}
\begin{split}
  \vy = \MA \, (\vx - \vm) + \vb.
\end{split}
\nonumber
\end{equation}

\item One option is to use \alert{Taylor series} centered at $\vx = \vm$, which leads to
%
\begin{equation}
\begin{split}
  \MA &= \MG_x(\vm), \\
  \vb &= \vg(\vm).
\end{split}
\nonumber
\end{equation}
%
\item Leads to the \alert{extended Kalman filter} (and smoother).

\item But this is \alert{not the only option} to choose $\MA$ and $\vb$.
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{Statistical linearization}

\begin{itemize}[<+->]
\item Instead, consider \alert{minimization of the error}
%
\begin{equation}
\begin{split}
  \mathrm{MSE}(\MA,\vb) = \E\left[ \| \vg(\vx) - \MA \, \vx - \vb \|^2 \right]
\end{split}
\nonumber
\end{equation}
%
%where $\E[]$ is
w.r.t. some $\vx \sim p(\vx)$ with $\E[\vx] = \vm$ and $\Cov[\vx] = \MP$.

\item We get
%
\begin{equation}
\begin{split}
  &\| \vg(\vx) - \MA \, (\vx - \vm) - \vb\|^2 \\
  &= \vg^\T(\vx)\, \vg(\vx) 
  - 2 \, \vg^\T(\vx) \, \MA \, (\vx - \vm) + \tr \left\{ \MA \, (\vx - \vm) \, (\vx - \vm)^\T \, \MA^\T \right\} \\
  &\qquad + 2 \, (\vx - \vm)^\T \MA^\T \vb
    - 2 \vg^\T(\vx) \, \vb + \vb^\T \vb.
\end{split}
\nonumber
\end{equation}

\item Taking \alert{expectation} gives
%
\begin{equation}
\begin{split}
  &\E\left[ \| \vg(\vx) - \MA \, (\vx - \vm) - \vb\|^2 \right] 
  = \E\left[ \vg^\T(\vx) \, \vg(\vx)\right] \\
  &- 2 \, \E\left[ \vg^\T(\vx) \, \MA \, (\vx - \vm) \right] + \tr\left\{ \MA \, \MP \, \MA^\T \right\} 
  - 2 \, \E\left[ \vg^\T(\vx)\right] \, \vb + \vb^\T \vb.
\end{split}
\nonumber
\end{equation}
%
\end{itemize}
\end{frame}


\begin{frame}
 \frametitle{Statistical linearization (cont.)}

\begin{itemize}[<+->]
\item Let us \alert{set the derivatives} w.r.t. $\vb$ and $\MA$ to \alert{zero}:
%
\begin{equation}
\begin{split}
  \frac{\partial \mathrm{MSE}}{\partial \vb} &= - 2 \, \E\left[ \vg(\vx) \right] + 2 \, \vb = 0, \\
  \frac{\partial \mathrm{MSE}}{\partial \MA} &= -2 \, \E\left[ \vg(\vx) \, (\vx - \vm)^\T \right] + 2 \, \MA \, \MP = 0.
\end{split}
\nonumber
\end{equation}
%
\item This leads to the \alert{optimal linearization parameters}
%
\begin{equation}
\begin{split}
  \vb &= \E\left[ \vg(\vx) \right], \\
  \MA &= \E\left[ \vg(\vx) \, (\vx - \vm)^\T \right] \, \MP^{-1} \\
  &= \E\left[ (\vg(\vx) - \vb) \, (\vx - \vm)^\T \right] \, \MP^{-1}.
\end{split}
\nonumber
\end{equation}

\item Thus the \alert{linearization} is (with the above $\vb$ and $\MA$)
%
\begin{equation}
\begin{split}
  \vy = \MA \, (\vx - \vm) + \vb.
\end{split}
\nonumber
\end{equation}

\item This is called classical \alert{statistical linearization (SL)}.
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{Statistical linearization: properties}

\begin{itemize}[<+->]
\item The \alert{expectation} of SL is indeed \alert{exact}:
%
\begin{equation}
\begin{split}
  \E\left[ \MA \, (\vx - \vm) + \vb \right] = \E\left[ \vg(\vx) \right].
\end{split}
\nonumber
\end{equation}

\item The \alert{covariance is not exact}:
%
\begin{equation}
\begin{split}
  &\E\left[ (\MA \, (\vx - \vm) + \vb - \vb) \, (\MA \, (\vx - \vm) + \vb - \vb)^\T \right] \\
  &= \E\left[ \MA \, (\vx - \vm) \, (\vx - \vm)^\T \MA^\T \right] \\
  &= \MA \, \MP \, \MA^\T \ne \Cov\left[ \vg(\vx) \right]  \qquad  \{ \text{ in general } \}.
\end{split}
\nonumber
\end{equation}

\item The corresponding SL filter and smoother have the \alert{same limitations}.

\item \alert{But we can fix this!}

\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Statistical linear regression}

\begin{frame}
 \frametitle{Statistical linear regression}

\begin{itemize}[<+->]
\item We can now replace the deterministic approximation on $\vg(\vx)$ with \alert{stochastic approximation}
%
\begin{equation}
\begin{split}
  \vy = \MA \, \vx + \vb + \ve,
\end{split}
\nonumber
\end{equation}
%
where $\ve \sim \N(0,\MSigma)$ is a \alert{pseudo-noise}.

\item The \alert{covariance} is now
%
\begin{equation}
\begin{split}
  &\E\left[ (\MA \, (\vx - \vm) + \vb + \ve - \vb) \, (\MA \, (\vx - \vm) + \vb + \ve - \vb)^\T \right] \\
  &= \MA \, \MP \, \MA^\T + \MSigma.
\end{split}
\nonumber
\end{equation}

\item Thus, we can \alert{force the correct covariance} by putting
%
\begin{equation}
\begin{split}
  \MSigma = \Cov\left[ \vg(\vx) \right] - \MA \, \MP \, \MA^\T.
\end{split}
\nonumber
\end{equation}

\item This is called \alert{statistical linear regression (SLR)}.
\end{itemize}
\end{frame}


\begin{frame}
 \frametitle{Statistical linear regression: prediction step}

\begin{itemize}[<+->]
\item Let us use the SLR approximation on the \alert{dynamic model}:
%
\begin{equation}
\begin{split}
  \vx_{k} = \vf(\vx_{k-1}) + \vq_{k-1},
\end{split}
\nonumber
\end{equation}
%
where $\vq_{k-1} \sim \N(0,\MQ)$. 

\item When in SLR we \alert{select $\vm = \vm_{k-1}$ and $\MP = \MP_{k-1}$} we get
%
\begin{equation}
\begin{split}
  \vx_{k} = \MF \, (\vx_{k-1} - \vm_{k-1}) + \vb + \ve + \vq_{k-1},
\end{split}
\nonumber
\end{equation}
%
with
%
\begin{equation}
\begin{split}
  \vb &= \E_{k-1}\left[ \vf(\vx_{k-1}) \right], \\
  \MF &= \E_{k-1}\left[ (\vf(\vx_{k-1}) - \vb) \, (\vx_{k-1} - \vm_{k-1})^\T \right] \, \MP_{k-1}^{-1}, \\
  \Cov_{k-1}[\ve] &= \Cov_{k-1}\left[ \vf(\vx_{k-1}) \right] - \MF \, \MP_{k-1} \, \MF^\T.
\end{split}
\nonumber
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{Statistical linear regression: prediction step (cont.)}

\begin{itemize}[<+->]
\item This now gives the \alert{prediction step}
%
\begin{equation}
\begin{split}
  \vm_k^- &= \E_{k-1}[ \vf(\vx_{k-1}) ], \\
  \MP_k^- &= \Cov_{k-1}[ \vf(\vx_{k-1}) ] + \MQ.
\end{split}
\nonumber
\end{equation}

\item The \alert{mean and covariance are exact} by construction.

\item This is indeed the \alert{Gaussian filter prediction} -- leading to UKFs and other sigma-point filters.
\end{itemize}
\end{frame}


\begin{frame}
 \frametitle{Statistical linear regression: update step}

\begin{itemize}[<+->]
\item For the \alert{measurement update} we consider
%
\begin{equation}
\begin{split}
  \vy_k = \vh(\vx_k) + \vr_k,
\end{split}
\nonumber
\end{equation}
%
where $\vr_k \sim \N(0,\MR)$.

\item \alert{Using SLR} with $\vm = \vm_k^-$ and $\MP = \MP_k^-$ gives
%
\begin{equation}
\begin{split}
  \vy_{k} = \MH \, (\vx_{k} - \vm_{k}^-) + \vmu_k + \ve'_{k} + \vr_{k},
\end{split}
\nonumber
\end{equation}
%
with
%
\begin{equation}
\begin{split}
 \vmu_k &= \E_k^-[ \vh(\vx_{k})], \\
  \MH    &= \E_k^-\left[ (\vh(\vx_{k}) - \vmu_k) \, (\vx_{k} - \vm_{k}^-)^\T \right] \, \Big[ \MP_{k}^- \Big]^{-1}, \\
  \Cov_k^-[\ve'] &= \Cov_k^-[ \vh(\vx_{k}) ] - \MH \, \MP_{k}^- \, \MH^\T.
\end{split}
\nonumber
\end{equation}
%
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{Statistical linear regression: update step (cont.)}

\begin{itemize}[<+->]
\item The \alert{joint mean and covariance} of $\vx_k$ and $\vy_k$ are given as
%
\begin{equation}
\begin{split}
  \E_k^-\left[ \begin{pmatrix} \vx_k \\ \vy_k \end{pmatrix} \right]
  &= \begin{pmatrix} \vm_k^- \\ \vmu_k \end{pmatrix} \\
  \Cov_k^-\left[ \begin{pmatrix} \vx_k \\ \vy_k \end{pmatrix} \right]
  &= \begin{pmatrix}
     \MP_k^-  & \E_k^-\left[ (\vx_{k} - \vm_{k}^-) \, (\vh(\vx_{k}) - \vmu_k)^\T \right] \\
      (\bullet)^\T & \Cov_k^-[ \vh(\vx_{k}) ] + \MR
      \end{pmatrix}
\end{split}
\nonumber
\end{equation}

\item \alert{Gaussian conditioning} leads to Gaussian filter update step
%
{\small
\begin{equation}
\begin{split}
  \vmu_k &= \E_k^-[ \vh(\vx_{k}) ], \\
   \MS_k &= \Cov_k^-[ \vh(\vx_{k}) ] + \MR, \\
   \MC_k &= \E_k^-\left[ (\vx_{k} - \vm_{k}^-) (\vh(\vx_{k}) - \vmu_k)^\T \right], \\
   \MK_k &= \MC_k \, \MS_k^{-1}, \\
   \vm_k &= \vm_k^- + \MK_k \, (\vy_k - \vmu_k), \\
   \MP_k &= \MP_k^- - \MK_k \, \MS_k \, \MK_k^\T.
\end{split}
\nonumber
\end{equation}
}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion and connections}

\begin{frame}
\frametitle{Discussion and connections}

\begin{itemize}[<+->]
\item Statistical linear regression leads to the \alert{Gaussian filter} -- and to UKF, CKF, GHKF, etc.

\item The corresponding \alert{smoother} can be derived similarly.

\item We have \alert{arbitrarily chosen} to do the SLR \alert{w.r.t. priors} $\N(\vm_{k-1},\MP_{k-1})$ and $\N(\vm_k^-,\MP_k^-)$ -- 

\begin{itemize}[<+->]
\item We don't need to do that, we can use any $\vx \sim p(\vx)$.
\item Leads to e.g. posterior-linearization filters and smoothers that \'Angel will talk about next.
\end{itemize}

\item \alert{Statistical linearization} corresponds to putting $\ve = 0$ in 
%
\begin{equation}
\begin{split}
  \vy = \MA \, \vx + \vb + \ve.
\end{split}
\nonumber
\end{equation}

\item We recover the \alert{statistically linearized filter}.

\item If we do the statistical linearization with \alert{prior means} and take $\MP_k \to 0$, $\MP_k^-
\to 0$ we obtain the \alert{EKF}.

\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

\begin{frame}{Summary}
\begin{itemize}[<+->]
\item In \alert{statistical linearization (SL)} we approximate $\vy = \vg(\vx)$ as $\vy = \MA \, (\vx - \vm) + \vb$, where $\MA$ and $\vb$ are selected to minimize
%
\begin{equation}
\begin{split}
  \mathrm{MSE}(\MA,\vb) = \E\left[ \| \vg(\vx) - \MA \, \vx - \vb \|^2 \right].
\end{split}
\nonumber
\end{equation}

\item In \alert{statistical linear regression (SLR)} we use $\vy = \MA \, \vx + \vb + \ve$, where $\ve$ is a Gaussian pseudo-noise that makes the covariance exact.

\item  By forming a filter with SLR we get \alert{Gaussian filter, UKFs, and other related filters}.

\item By using SL we get the classical \alert{statistically linearized filter} and also \alert{EKF} as a special case.

\item We can also do SL and SLR w.r.t. \alert{other than the prior} -- leading to posterior linearization filters (and smoothers).
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
